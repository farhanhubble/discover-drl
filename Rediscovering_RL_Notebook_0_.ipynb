{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rediscovering RL- Notebook 0 .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNB8HZyPlqUmpsz7eX1+VXI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farhanhubble/discover-drl/blob/master/Rediscovering_RL_Notebook_0_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv9hgPks9LhM",
        "colab_type": "text"
      },
      "source": [
        "# Reinforcement learning with Foolsball\n",
        "- Reinforcement learning is learning to make decisions from experience.\n",
        "- Games are a good testbed for agents to interaction with an environment and explore it.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWEnCb0m9ve8",
        "colab_type": "text"
      },
      "source": [
        "# About Foolsball\n",
        "- 5x4 playground that provides a football/foosball-like environment.\n",
        "- An agent or actor:\n",
        "  - always spawned in the top-left corner\n",
        "  - displayed as 'âš½'\n",
        "  - can move North, South, East or West.\n",
        "  - can be controlled algorithmically\n",
        "- A number of **static** opponents, each represented by ðŸ‘•, that occupy certain locations on the field.\n",
        "- A goalpost ðŸ¥… that is fixed in the bottom right corner\n",
        "\n",
        "## Primary goal\n",
        "- We want the agent to learn to reach the goalpost \n",
        "\n",
        "## Secondary goals\n",
        "- We may want the agent to learn to be efficient in some sense, for example, take the shortest path to the goalpost. **More precisely we want an algorithm to learn to control the agent and steer it towards the goalpost.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcMMYrax7MVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent = 'âš½'\n",
        "opponent = 'ðŸ‘•'\n",
        "goal = 'ðŸ¥…'\n",
        "\n",
        "arena = [['âš½', ' ' , 'ðŸ‘•', ' ' ],\n",
        "         [' ' , ' ' , ' ' , 'ðŸ‘•'],\n",
        "         [' ' , 'ðŸ‘•', ' ' , ' ' ],\n",
        "         [' ' , ' ' , ' ' , 'ðŸ‘•'],\n",
        "         [' ' , 'ðŸ‘•', ' ' , 'ðŸ¥…']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKaYR3mmAc6o",
        "colab_type": "text"
      },
      "source": [
        "# Implementing an environment for the game of Foolsball\n",
        "- OpenAI Gym has many [text environments](https://github.com/openai/gym/tree/master/gym/envs/toy_text)\n",
        "- Text environments are simple to render in a notebook and super-fast to experiment with.\n",
        "- We want to build our own environment for two reasons:\n",
        "  - It's a great exercise in understanding the finer details, like states, actions, rewards, returns.\n",
        "  - Some of the experimentation we do requires looking under the hood of the environment, which is easier with your own implementation than OpenAI Gym.\n",
        "  - OpenAI Gym has a simple `step(), reset()` API that we also implement. So porting our implentation over to Gym shoud be easy (and fun)!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyjTlB4cCysH",
        "colab_type": "text"
      },
      "source": [
        "# Understanding the first bits of terminology.\n",
        "## State \n",
        "- In RL state refers to information about the environemnt and the agent.\n",
        "- An RL algorithm inspects the state to decide which action to take.\n",
        "- Exactly what information gets captured in `state` depends on a few factors:\n",
        "  - The complexity of the environment: \n",
        "    - The number of actors, \n",
        "    - the nature of the environment, for example text or images. \n",
        "  - The complexity of the algorithm\n",
        "    - A simple algorithm may only need information about the agent and its immediate surroundings.\n",
        "    - A more complex algorithm may need information about the whole environment.\n",
        "\n",
        "\n",
        "## Setup\n",
        "- In our case we want the algorithm to only know about the location of the agent on the field. \n",
        "- We could have included information about the opponents too which would perhaps aid in the decision making but we chose not to.  \n",
        "\n",
        "- The state therefore is a tuple: (row, col), representing the location of the agent. \n",
        "- There are 20 possible values that `state` can take on:\n",
        "  - `row` can range from 0 through 4\n",
        "  - `col` can range from 0 through 3\n",
        "\n",
        "## Implementation details\n",
        "- The state is actually stored as a single integer that can take on values between 0 and 19."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5auCn-_GLVeU",
        "colab_type": "text"
      },
      "source": [
        "## Actions\n",
        "The agents can perfrom actions in an environment.\n",
        "\n",
        "## Setup\n",
        "- Our agent can perform one kind of action: navigate up, down, right or left.\n",
        "- It has 4 actions: 'n', 'e', 'w', 's'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjxD0m0uMfbU",
        "colab_type": "text"
      },
      "source": [
        "# Learning from experience\n",
        "Any RL set up can be modeled as shown below:\n",
        "\n",
        "![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcTMDmrmnl_dAyjCOErHPak2gLXmQTgQnVT8gQ&usqp=CAU)\n",
        "\n",
        "- The agent performs an action in the environment\n",
        "- The state of the environment and agent change as a result\n",
        "- The agent receives a reward and the updated state from the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASC5S75zN8mO",
        "colab_type": "text"
      },
      "source": [
        "## Rewards\n",
        "- Reward is the signal that an agent receives after it performs an action.\n",
        "- The reward structure has to be decided by us. \n",
        "- The biggest challenge of RL is that reward is often sparse. \n",
        "\n",
        "## Set up\n",
        "- In our case the reward depends on the rules of the game and the goal.\n",
        "  - If the agent runs into an opponent, the game gets over and the reward is negative (penalizes the agent)\n",
        "  - If the agent makes it to the goalpost, the game gets over and the reward is positive.\n",
        "  - if the agent takes the ball out of the field the reward is negative.\n",
        "  - If the agent makes a valid move what shoud the reward be?\n",
        "\n",
        "## Implementation\n",
        "- The default reward structure in our case is  `{'unmarked':-1, 'opponent':-5, 'outside':-1, 'goal':+5}`\n",
        "- This can be changed at any time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2--uKoiUHts",
        "colab_type": "text"
      },
      "source": [
        "#Let's start!\n",
        "---\n",
        "# Step 1\n",
        "The code below provides an skeleton for the **Foolsball** environment we want to our agent to train in. Fill in the code marked with #Todo to create a working environment.\n",
        "\n",
        "1. Go to the `__init__()` method and try to understand what it is doing\n",
        "  1. Look at the deserialize method and complete all todos.\n",
        "2. Complete the `__to_state_()` and `__to_indices__()` methods.\n",
        "3. Complete the `reset()` method.\n",
        "4. Go to the `step()` method and understand its intended behavior.\n",
        "  1. Complete `__get_next_state_on_action__()`\n",
        "  2. Complete `__get_reward_for_transition__()`\n",
        "  3. Complete the `step()` method.\n",
        "\n",
        "\n",
        "5. Read through the `render()` function to understand how we display the environment in the different situations. \n",
        "\n",
        "6. Execute the cell below and make sure there are no errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faVO8xdj7ZLA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Foolsball(object):\n",
        "\n",
        "  def __to_state__(self,row,col):\n",
        "    \"\"\"Convert from integer state to indices (row,col).\"\"\"\n",
        "    return #Todo\n",
        "\n",
        "  def __to_indices__(self, state):\n",
        "    \"\"\"Convert indices(row,col) to state (single integer).\"\"\"\n",
        "    row = #Todo\n",
        "    col = #Todo\n",
        "    return row,col\n",
        "\n",
        "  def __deserialize__(self,map:list,agent:str,opponent:str, goal:str):\n",
        "    \"\"\"Convrt a string representation of a map into a 2D numpy array\n",
        "    Param map: list of lists of strings representing the player, opponents and goal.\n",
        "    Param agent: string representing the agent on the map \n",
        "    Param opponent: string representing every instance of an opponent player\n",
        "    Param goal: string representing the location of the goal on the map\n",
        "    \"\"\"\n",
        "    ## Capture dimensions and map.\n",
        "    self.n_rows = #Todo\n",
        "    self.n_cols = #Todo\n",
        "    self.n_states = #Todo \n",
        "    self.map = #Todo: convert map to Numpy array\n",
        "\n",
        "    ## Store string representations for printing the map, etc.\n",
        "    self.agent_repr = #Todo\n",
        "    self.opponent_repr  = #Todo\n",
        "    self.goal_repr = #Todo\n",
        "\n",
        "    ## Find initial state, the desired goal state and the state of the opponents. \n",
        "    self.init_state = None\n",
        "    self.goal_state = None\n",
        "    self.opponents_states = []\n",
        "\n",
        "    for row in range(self.n_rows):\n",
        "      for col in range(self.n_cols):\n",
        "\n",
        "        if map[row][col] == agent:\n",
        "          # Store the initial state outside the map.\n",
        "          # This helps in quickly resetting the game to the initial state and\n",
        "          # also simplifies printing the map independent of the agent's state. \n",
        "          self.init_state = #Todo\n",
        "          self.map[row,col] = ' ' \n",
        "        \n",
        "        elif map[row][col] == opponent:\n",
        "          #Todo\n",
        "\n",
        "        elif map[row][col] == goal:\n",
        "          #Todo\n",
        "\n",
        "    assert self.init_state is not None, print(f\"Map {map} does not specify an agent {agent} location\")\n",
        "    assert self.goal_state is not None,  print(f\"Map {map} does not specify a goal {goal} location\")\n",
        "    assert self.opponents_states,  print(f\"Map {map} does not specify any opponents {opponent} location\")\n",
        "\n",
        "    return self.init_state\n",
        "\n",
        "\n",
        "  def __get_next_state_on_action__(self,state,action):\n",
        "    \"\"\"Return next state based on current state and action.\"\"\"\n",
        "    row, col = self.__to_indices__(state)\n",
        "    action_to_index_delta = {'n':[-1,0], 'e':[0,+1], 'w':[0,-1], 's':[+1,0]}\n",
        "\n",
        "    row_delta, col_delta = action_to_index_delta[action]\n",
        "    new_row , new_col = row+row_delta, col+col_delta\n",
        "\n",
        "    ## Return current state if next state is invalid\n",
        "    if #Todo: add proper condition\n",
        "      return state  \n",
        "\n",
        "    ## Construct state from new row and col and return it.    \n",
        "    return #Todo\n",
        "\n",
        "\n",
        "  def __get_reward_for_transition__(self,state,next_state):\n",
        "    \"\"\" Return the reward based on the transition from current state to next state. \"\"\"\n",
        "    ## Transition rejected due to illegal action (move)\n",
        "    if next_state == state:\n",
        "      reward = #Todo\n",
        "    \n",
        "    ## Goal!\n",
        "    elif next_state == self.goal_state:\n",
        "      reward = #Todo\n",
        "    \n",
        "    ## Ran into opponent. \n",
        "    elif next_state in self.opponents_states:\n",
        "      reward = #Todo\n",
        "\n",
        "    ## Made a safe and valid move.   \n",
        "    else:\n",
        "      reward = #Todo\n",
        "\n",
        "    return reward\n",
        "\n",
        "\n",
        "  def __is_terminal_state__(self, state):\n",
        "    return (state == self.goal_state) or (state in self.opponents_states) \n",
        "\n",
        "  \n",
        "  def __init__(self,map,agent,opponent,goal):\n",
        "    \"\"\"Spawn the world, create variables to track state and actions.\"\"\"\n",
        "    # We just need to track the location of the agent (the ball)\n",
        "    # Everything else is static and so a potential algorithm doesn't \n",
        "    # have to look at it. The variable `done` flags terminal states.\n",
        "    self.state = self.__deserialize__(map,agent,opponent,goal)\n",
        "    self.done = False\n",
        "    self.actions = ['n','e','w','s']\n",
        "\n",
        "    # Set up the rewards\n",
        "    self.default_rewards = {'unmarked':-1, 'opponent':-5, 'outside':-1, 'goal':+5}\n",
        "    self.set_rewards(self.default_rewards)\n",
        "\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Reset the environment to its initial state.\"\"\"\n",
        "    # There's really just two things we need to reset: the state, which should\n",
        "    # be reset to the initial state, and the `done` flag which should be \n",
        "    # cleared to signal that we are not in a terminal state anymore, even if we \n",
        "    # were earlier. \n",
        "    self.state = #Todo: set to initial state \n",
        "    self.done  = #Clear the flag\n",
        "    return self.state\n",
        "\n",
        "  \n",
        "  def set_rewards(self,rewards):\n",
        "    if not self.state == self.init_state:\n",
        "      print('Warning: Setting reward while not in initial state! You may want to call reset() first.')\n",
        "    for key in self.default_rewards:\n",
        "      assert key in rewards, print(f'Key {key} missing from reward.') \n",
        "    self.rewards = rewards\n",
        "\n",
        "  \n",
        "  def step(self,action):\n",
        "    \"\"\"Simulate state transition based on current state and action received.\"\"\"\n",
        "    assert not self.done, \\\n",
        "    print(f'You cannot call step() in a terminal state({self.state}). Check the \"done\" flag before calling step() to avoid this.')\n",
        "    next_state = #Todo: Get next state for this (state, action) pair\n",
        "\n",
        "    reward = #Todo: Get the reward for the state -> next_state transition.\n",
        "\n",
        "    done = #Todo set the flag if we are in a terminal state\n",
        "\n",
        "    self.state, self.done = next_state, done\n",
        "    \n",
        "    return next_state, reward, done\n",
        "\n",
        "\n",
        "  def render(self):\n",
        "    \"\"\"Pretty-print the environment and agent.\"\"\"\n",
        "    ## Create a copy of the map and change data type to accomodate\n",
        "    ## 3-character strings\n",
        "    _map = np.array(self.map, dtype='<U3')\n",
        "\n",
        "    ## Mark unoccupied positions with special symbol.\n",
        "    ## And add extra spacing to align all columns.\n",
        "    for row in range(_map.shape[0]):\n",
        "      for col in range(_map.shape[1]):\n",
        "        if _map[row,col] == ' ':\n",
        "          _map[row,col] = ' + '\n",
        "        \n",
        "        elif _map[row,col] == self.opponent_repr: \n",
        "          _map[row,col] =  self.opponent_repr + ' '\n",
        "        \n",
        "        elif _map[row,col] == self.goal_repr:\n",
        "          _map[row,col] = ' ' + self.goal_repr + ' '\n",
        "      \n",
        "    ## If current state overlaps with the goal state or one of the opponents'\n",
        "    ## states, susbstitute a distinct marker.\n",
        "    if self.state == self.goal_state:\n",
        "      r,c = self.__to_indices__(self.state)\n",
        "      _map[r,c] = ' ðŸ '\n",
        "    elif self.state in self.opponents_states:\n",
        "      r,c = self.__to_indices__(self.state)\n",
        "      _map[r,c] = ' â— '\n",
        "    else:\n",
        "      r,c = self.__to_indices__(self.state)\n",
        "      _map[r,c] = ' ' + self.agent_repr\n",
        "    \n",
        "    for row in range(_map.shape[0]):\n",
        "      for col in range(_map.shape[1]):\n",
        "        print(f' {_map[row,col]} ',end=\"\")\n",
        "      print('\\n') \n",
        "    \n",
        "    print()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWXHSX9IWm9o",
        "colab_type": "text"
      },
      "source": [
        "# Step 2\n",
        "Execute the two cell below and ensure that there are no runtime error and the rendering happens correctly. You should see output like this:\n",
        "\n",
        "```\n",
        "  âš½   +   ðŸ‘•    +  \n",
        "\n",
        "  +    +    +   ðŸ‘•  \n",
        "\n",
        "  +   ðŸ‘•    +    +  \n",
        "\n",
        "  +    +    +   ðŸ‘•  \n",
        "\n",
        "  +   ðŸ‘•    +    ðŸ¥…  \n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hK5uEU1QGddR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "foolsball = Foolsball(arena, agent, opponent, goal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nn8RNR1NDZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "foolsball.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjQffds2YGlj",
        "colab_type": "text"
      },
      "source": [
        "# Step 3.\n",
        "- Run the next cell to play with the environment and score a few goals. \n",
        "- If there are any errors you may want to go back and update the code for the `Foolsball` class. \n",
        "- Make sure to run the cell with `foolsball = Foolsball(arena, agent, opponent, goal)` if you update the class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4w1HCRHwG60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Move: n,s,e,w\n",
        "## Reset: r\n",
        "## Exit: x\n",
        "while True:\n",
        "  try:\n",
        "    act = input('>>')\n",
        "\n",
        "    if act in foolsball.actions:\n",
        "      print(foolsball.step(act))\n",
        "      print()\n",
        "      foolsball.render()\n",
        "    elif act == 'r':\n",
        "      print(foolsball.reset())\n",
        "      print()\n",
        "      foolsball.render()\n",
        "    elif act == 'x':\n",
        "      break\n",
        "    else:\n",
        "      print(f'Invalid input:{act}')\n",
        "  except Exception as e:\n",
        "    print(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0Fx2b9DdMxh",
        "colab_type": "text"
      },
      "source": [
        "# Step 4\n",
        "Understand the concept of returns\n",
        "- Complete the `get_return()` function.\n",
        "- Calculate returns for a few sample paths by running the next few cells"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyzJFjD5FRov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Reward and return\n",
        "path1 = ['e','s','e','s','s','s','e']\n",
        "path2 = ['s','e','e','s','s','s','e']\n",
        "path3 = ['s','s','s','e','e','s','e']\n",
        "path4 = ['s','s','s','s','n','e','e','s','e']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJqLJA8jQCyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_return(path):\n",
        "  # Todo: Reset the game to its initial state\n",
        "  # Todo: Render the starting state\n",
        "  \n",
        "  _return_ = 0\n",
        "  for act in path: \n",
        "    # Todo: use the step() API to run the action.\n",
        "    # Todo: accumulate reward in to return.\n",
        "    # Todo: render the current state (purely for visual delight)\n",
        "    \n",
        "    if #Todo: condition to break out.\n",
        "      break\n",
        "    \n",
        "  print(f'Return (accumulated reward): {_return_}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyqzhRJeR3eL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_return(path1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kGOePDNR7vz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_return(path2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyqLBXy0Sncw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_return(path3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFJS37teSpQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_return(path4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bBwFGPFe9Ld",
        "colab_type": "text"
      },
      "source": [
        "# Step 5.\n",
        "- Experiment with a different reward structure.\n",
        "- Does it encourage the agent to take the shortest route?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdzQu5qmStXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Different reward structure\n",
        "foolsball.set_rewards({'unmarked':0, 'opponent':-5, 'outside':-1, 'goal':+5})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXTnL0IPVO1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_return(path1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RdLqt9FVcCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_return(path4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEQw1TCBfVXa",
        "colab_type": "text"
      },
      "source": [
        "# Step 6\n",
        "- Get introduced to discounted return as a means to set acceptable time horizons.\n",
        "$$Discounted\\ Return = R_{t_1} + \\gamma*R_{t_2} + \\gamma^2*R_{t_3} + ... + \\gamma^{n-1}*R_{t_n}$$\n",
        "where $R_{t_k}$  is the reward after step `k` and $\\gamma$ is called the discount factor. \n",
        "- Complete the code below to implement discounted returns.\n",
        "- The discount factor $\\gamma$ is a hyperparameter (why?) often set to 0.9 \n",
        "ðŸ˜œ\n",
        "- Run the next few cells to see if discounting indeed has the effect we want (shorter paths)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csaEvig4WuCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_discounted_return(path, gamma=0):\n",
        "  foolsball.reset()\n",
        "  foolsball.render()\n",
        "  _return_ = 0\n",
        "  discount_coeff = 1\n",
        "  for act in path: \n",
        "    #Todo: execute one step\n",
        "    #Todo: Update discounted reward\n",
        "    #Todo: Update the discount multiplier pow(gamma,n-1)\n",
        "    \n",
        "    foolsball.render()\n",
        "    if done:\n",
        "      break\n",
        "    \n",
        "  print(f'Return (accumulated reward): {_return_}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBuFLEMLEUbr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HYPER_PARAMS = {'gamma':0.9}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StLUFUgjW_cz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_discounted_return(path1, HYPER_PARAMS['gamma'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtaApbodXVgp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_discounted_return(path4, HYPER_PARAMS['gamma'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cibo9KfQiBxm",
        "colab_type": "text"
      },
      "source": [
        "# Step 7\n",
        "## Formalizing the problem:\n",
        "- We want to the agent to reach the goalpost AND attain the highest **discounted return**.\n",
        "- This means making safe and efficient moves\n",
        "  - Running into opponent means game over\n",
        "  - Repeated 'outsides' means inefficiency\n",
        "  - Long detours are also inefficient\n",
        "\n",
        "## The Conundrum\n",
        "- We already know how to compute the discounted return from a path.\n",
        "- We can generate all possible paths and calculate their returns and pick a path with the highest return.\n",
        "\n",
        "- Alas there are too many paths (4 possible decision at each step)\n",
        "\n",
        "\n",
        "## The \"Trick\"\n",
        "- Even though there are too many paths, the number of (state,action) pair is small.\n",
        "- We can calculate the return for each of the 80(=20x4) state action pairs.\n",
        "- To emphasize we want to caculate the return for each (state,action) pair not the reward\n",
        "  - Calculating return means peekin into the future.\n",
        "\n",
        "\n",
        "## Todo:\n",
        "- As a precursor to calculating returns for every (state,action) pair let's try to calculate the reward for every (state,action) pair.\n",
        "\n",
        "- Understand how the code in the next two cells creates a Pandas table to store the rewards for every (state, action) pair.\n",
        "\n",
        "- We will cheat a little by using private methods of the `Foolsball` class\n",
        "  - Use the `__get_next_state_on_action__()` and `__get_reward_for_transition__()` methods to complete the code in the third cell below\n",
        "  - Run the fourth cell to view the rewards table. \n",
        "  - Notice that rewards for terminal states are kept undefined since no actions are allowed in those states.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNH54KMKpvua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yascBBM5uLwC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "REWARDS_TBL = pd.DataFrame.from_dict({s:{a:None for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
        "REWARDS_TBL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJRlz7nBuQxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for state in REWARDS_TBL.index:\n",
        "  if not foolsball.__is_terminal_state__(state): #Only calculate rewards for non-terminal states\n",
        "    for action in REWARDS_TBL.columns:\n",
        "      next_state = #Todo get the next state for this state,action \n",
        "      REWARDS_TBL.loc[state, action] = #Todo get the reward for state -> new_state transition "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u-rbqyavKWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "terminal_states = foolsball.opponents_states+[foolsball.goal_state]\n",
        "print(terminal_states)\n",
        "REWARDS_TBL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtEnjtynnkaI",
        "colab_type": "text"
      },
      "source": [
        "#Step 8\n",
        "Create a returns table (no TODOs here)\n",
        "- Run the next four cells and understand why we are setting the returns for terminal stated to 0.\n",
        "  - We leave the returns for all non-terminal states undefined.\n",
        "  - Trying to fill up these entries will be the focus of the rest of the notebook.\n",
        "\n",
        "- A function to create new instances of the returns table is also provided in the fourth cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiWIpBIj0sql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RETURNS_TBL = pd.DataFrame.from_dict({s:{a:None for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGFzW7Xb886q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RETURNS_TBL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKCqzD9_8-E7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RETURNS_TBL.loc[terminal_states]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVay2Rer9eQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RETURNS_TBL.loc[terminal_states] = 0\n",
        "RETURNS_TBL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft8yVJdJWd_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_returns_table(terminal_states):\n",
        "  \"\"\"Create an empty returns table.\"\"\"\n",
        "  table = pd.DataFrame.from_dict({s:{a:None for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
        "  table.loc[terminal_states] = 0\n",
        "  return table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXXnzbJEpDuY",
        "colab_type": "text"
      },
      "source": [
        "# Step 9\n",
        "## Try dynamic programming to fill up the returns table.\n",
        "- Returns for a (state, action) are defined in terms of returns of the next state. \n",
        "  - $Return(state_t,action_t) = Reward(state_t,state_{t+1}) + \\max[Return(state_{t+1}, action=='n'),\\\\ Return(state_{t+1}, action=='e'), \\\\ Return(state_{t+1}, action=='w'), \\\\ Return(state_{t+1}, action=='s')]$\n",
        "\n",
        "  - This motivates the use of dynamic programming to fill up the returns table  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLFtYd-AtsgI",
        "colab_type": "text"
      },
      "source": [
        "## Todo:\n",
        "- Read the code in the next cell and try to understand the first dynamic programming based solution. \n",
        "- Run the code in the next cell. The code causes a stack overflow. Why?\n",
        "- Pass debug= True to see what the problem is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZKZKPpYM8FD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fill_returns_table_v0(table,state,debug=False): \n",
        "  \"\"\" Recursively fill a returns table, one state at a time.\"\"\"\n",
        "  for action in table.columns:\n",
        "    if table.loc[state][action] is None:\n",
        "      next_state = foolsball.__get_next_state_on_action__(state, action)\n",
        "      reward = foolsball.__get_reward_for_transition__(state, next_state)\n",
        "\n",
        "      if debug:\n",
        "        print(f'Trying to fill ({state},{action},{next_state})')\n",
        "      \n",
        "      fill_returns_table_v0(table, next_state, debug) # <= Earth shaking problem here!!! ðŸ˜±ðŸ˜±ðŸ˜±\n",
        "      table.loc[state][action]  = reward + HYPER_PARAMS['gamma'] * table.loc[next_state].max()\n",
        "    \n",
        "    else:\n",
        "      if debug:\n",
        "        print((state,action),f'already has a RETURN {table.loc[state][action]}')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrWAG02iNMkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table = make_returns_table(terminal_states)\n",
        "fill_returns_table_v0(table,state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_O2GDu7uTAw",
        "colab_type": "text"
      },
      "source": [
        "## Contd..\n",
        "- The code above crashed becasue of indefinite recursion caused by a state,action pairs that resulted in the next state being the same as the current state\n",
        "- We can fix this by catching this case and a returning a large negative return.\n",
        "- Why is the large negative return necessary?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFbLxTZ0Gy64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fill_returns_table_v1(table,state,debug=False):\n",
        "  for action in table.columns:\n",
        "    if table.loc[state][action] is None:\n",
        "      next_state = foolsball.__get_next_state_on_action__(state, action)\n",
        "      reward = foolsball.__get_reward_for_transition__(state, next_state)\n",
        "      \n",
        "      if debug:\n",
        "        print(f'Trying to fill ({state},{action},{next_state})')\n",
        "      \n",
        "      if next_state == state:\n",
        "        table.loc[state][action] = -np.inf # <= No self recursion\n",
        "      else:\n",
        "        fill_returns_table_v1(table,next_state,debug)\n",
        "        table.loc[state][action]  = reward + HYPER_PARAMS['gamma'] * table.loc[next_state].max()\n",
        "    else:\n",
        "      if debug:\n",
        "        print((state,action),f'already has a RETURN {table.loc[state][action]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1AXgzoQInII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table = make_returns_table(terminal_states)\n",
        "fill_returns_table_v1(table, state=0, debug=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLfbLuhtvbKe",
        "colab_type": "text"
      },
      "source": [
        "## Contd..\n",
        "- The code above crashed becasue of indefinite mutual recursion caused by a state,action pairs that resulted in the next state being the same as the current state\n",
        "- We can fix this by evading these cases.\n",
        "- Let' see if we can get somewhere.\n",
        "- Run the next few cells to find out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbpZkdIwJTq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fill_returns_table_v2(table,state, debug=False):\n",
        "  for action in table.columns:\n",
        "    if table.loc[state][action] is None:\n",
        "      next_state = foolsball.__get_next_state_on_action__(state, action)\n",
        "      reward = foolsball.__get_reward_for_transition__(state, next_state)\n",
        "      \n",
        "      if debug:\n",
        "        print(f'Trying to fill ({state},{action},{next_state})')\n",
        "      \n",
        "      if next_state == state:\n",
        "        table.loc[state][action] = -np.inf # <= No self recursion\n",
        "      \n",
        "      elif not table.loc[next_state].isna().any(): # <= No recursion beyond immediate neighbor!\n",
        "        table.loc[state][action]  = reward + HYPER_PARAMS['gamma'] * table.loc[next_state].max()\n",
        "    \n",
        "    else:\n",
        "      if debug:\n",
        "        print((state,action),f'already has a RETURN {table.loc[state][action]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mm49G7-LVq83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table = make_returns_table(terminal_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1dniXqKZRIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fill_returns_table_v2(table,state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDThbMqxZYrl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4J8aE8qZaPR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fill_returns_table_v2(table,state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcpD53bxi4js",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvzzuhegi5aT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fill_returns_table_v2(table,state=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HkIB9Pzj7Ra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBT7-eE-j8ja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for s in range(4,19):\n",
        "  fill_returns_table_v2(table,state=s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfjVh9OAkOrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ierdKTiOklYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwRKvA9rkRBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for s in range(0,19):\n",
        "  fill_returns_table_v2(table,state=s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spKnLucvkhOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaO1U52hkiUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCi6KuKAkpvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for s in range(0,19):\n",
        "  fill_returns_table_v2(table,state=s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZx5QB-slAHK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gB4i_CZLlBci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSucaBX6wH7V",
        "colab_type": "text"
      },
      "source": [
        "# Get Some more Coffee\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad0G1-3_-pw0",
        "colab_type": "text"
      },
      "source": [
        "# Step 10\n",
        "## Estimating returns through simulation\n",
        "- and Monte Carlo sampling\n",
        "- No more cheating by peeping into the environment (private APIs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkfoziC6wndC",
        "colab_type": "text"
      },
      "source": [
        "## Todo\n",
        "- Run the code in the next two cells to collect and print a random episodes.\n",
        "  - The episode starts with the environment in the initial state \n",
        "  - The agent tries random actions\n",
        "  - The episode terminates when the agent collides with an opponent or reaches the goalpost.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ij_84xHhlFZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collect_random_episode():\n",
        "  state = foolsball.reset()\n",
        "  done = False\n",
        "  episode = []\n",
        "\n",
        "  while not done:\n",
        "    action = np.random.choice(foolsball.actions)\n",
        "    next_state, reward, done = foolsball.step(action)\n",
        "    episode.append([state, action, reward])\n",
        "    state = next_state\n",
        "  \n",
        "  return episode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK5xQ6iwvMU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ep = collect_random_episode()\n",
        "foolsball.render()\n",
        "print(ep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLbl3Dxmx122",
        "colab_type": "text"
      },
      "source": [
        "# Step 11\n",
        "- Complete the function `discounted_return_from_episode()` that computes the discounted return for every state in an episode.\n",
        "  - If an episode is:  $(s_1,a_1,r_1), (s_2,a_2,r_2), (s_3, a_3, r_3)$, **excluding the terminal state**:\n",
        "  - The (discounted) return for $s_1$ is $r_1 + \\gamma * r_2 + \\gamma^2 * r_3$\n",
        "  - The (discounted) return for $s_2$ is $r_2 + \\gamma * r_3$\n",
        "  - The (discounted) return for $s_3$ is $r_3$ \n",
        "\n",
        "- Run the next couple of cells to print discounted returns for entire episodes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_r_tTfSwEIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discounted_return_from_episode(ep, gamma=0):\n",
        "  states, actions, rewards = list(zip(*ep))\n",
        "  rewards = np.asarray(rewards)\n",
        "  discount_coeffs = np.asarray([np.power(gamma,p) for p in range(len(rewards))])\n",
        "  \n",
        "  l = len(rewards)\n",
        "  discounted_returns = [np.dot(rewards[\"\"\"#Todo:Fill appropriate range\"\"\"],discount_coeffs[\"\"\"#Todo:Fill appropriate range\"\"\") for i in range(l)]\n",
        "\n",
        "  return (states, actions, discounted_returns)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp9dvrQjl_vA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discounted_return_from_episode(ep, gamma=HYPER_PARAMS['gamma'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvgikRgT0FQ-",
        "colab_type": "text"
      },
      "source": [
        "# Step 12\n",
        "## Estimate returns by simulating lot of episodes.\n",
        "- The code below creates two tables:\n",
        "  - ESTIMATED_RETURNS_TBL for accumulating the return for every (state,action) pair  \n",
        "  - VISITS_COUNTS_TBL for storing the number of times a (state,action) pair appears across all episodes.\n",
        "\n",
        "- It then runs an algorithm to generate episodes and \n",
        "\n",
        "Here's the idea:\n",
        "- Create many random episodes\n",
        "  - Examine each (state, action) pair in an episode.\n",
        "  - Calculate and accumulate the return for this pair\n",
        "    - Since we have the full episode, we can \"see the future\" and calculate the return.\n",
        "    - The return for a (state,action) pair is just (very bad) estimate of the \"real\" return, since we are looking at just one of the many paths that could possible contain the (state,action)\n",
        "  - Record the visit count of the (state, action) pair.   \n",
        "\n",
        "- At the end the we divide the accumulated returns by the visit counts to get an estimate of the retruns. \n",
        "\n",
        "\n",
        "## Todo:\n",
        "- Complete the code in the **next two cells** to implement what's known as Monte Carlo estimation.\n",
        "- Run the cells to see how well the alorithm fares.\n",
        "- Does the algorithm generate a sensible looking returns (estimates)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1R--MLHmalx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create empty returns table \n",
        "ESTIMATED_RETURNS_TBL = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
        "VISITS_COUNTS_TBL = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
        "\n",
        "n_episodes = 100  #Try 100, 500, 2000\n",
        "\n",
        "for i in range(n_episodes):\n",
        "  episode_i = #Todo: Create a random episode.\n",
        "  states, actions, discounted_returns = #Todo: Generate discounted returns for the episode\n",
        "\n",
        "  for s,a,ret in zip(states, actions, discounted_returns):\n",
        "    ESTIMATED_RETURNS_TBL.loc[s,a] += #Todo: Accumulate the return for this (state,action) pair\n",
        "    VISITS_COUNTS_TBL.loc[s,a] +=  #Todo: update visit count\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GunNiOL6pC7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimated_returns = ESTIMATED_RETURNS_TBL.div(VISITS_COUNTS_TBL+1) ## Averaging returns. Avoid dividing by zeros.\n",
        "estimated_returns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtFqZ61zsoUb",
        "colab_type": "text"
      },
      "source": [
        "# Step 13:\n",
        "## Intro to Policies\n",
        "- The estimated returns table is hard to evaluate.\n",
        "- To use the table to make decisions, we grab the action with the highest returns.\n",
        "- We can extract the actions yielding the highest return in each state and call it a **policy**.\n",
        "- This will be a greedy policy since we take the best action at each state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLYn9_dr56pX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def greedy_policy_from_returns_tbl(table):\n",
        "  policy = {s:None for s in table.index }\n",
        "\n",
        "  for state in table.index:\n",
        "    if state not in terminal_states:\n",
        "      greedy_action_index = # Todo: get the index of the action with the highest return.\n",
        "      greedy_action = table.columns[greedy_action_index]\n",
        "      policy[state] = greedy_action\n",
        "\n",
        "  return policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zTVhU_d691b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "policy0 = greedy_policy_from_returns_tbl(estimated_returns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1w03qgvN7JF-",
        "colab_type": "text"
      },
      "source": [
        "# Contd..\n",
        "- Here's a function to superimpose a policy over the environment.\n",
        "- Use the code in the next two cells to eyeball the policy we just generated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzumT6HE6sq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pretty_print_policy(policy):\n",
        "  direction_repr = {'n':' ðŸ¡‘ ', 'e':' ðŸ¡’ ', 'w':' ðŸ¡ ', 's':' ðŸ¡“ ', None:' â¬¤ '}\n",
        "\n",
        "  for row in range(foolsball.n_rows):\n",
        "    for col in range(foolsball.n_cols):\n",
        "      state = row * foolsball.n_cols + col\n",
        "      print(direction_repr[policy[state]],end='')\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBaobgcs647k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretty_print_policy(policy0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hz2JFP929GfR",
        "colab_type": "text"
      },
      "source": [
        "# Step 14\n",
        "## Exploiting the information in the returns table.\n",
        "- We are improving our estimates of the returns with each successive episode. \n",
        "- But we are still generating random episodes throughout. \n",
        "- We should also exploit the information we accrue in returns table\n",
        "- The implementation below is quite similar to `collect_random_episode` but here's the key difference:\n",
        "  - In state s, the random policy returns a random action from ('n','s','e','w').\n",
        "  - But from the returns table we know that one of the action, say 'e' generates the best returns so we can make a greedy choice and always return 'e'\n",
        "\n",
        "- Run the next to cells to see the difference. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVIs2nQTj2cD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collect_greedy_episode_from_returns_tbl(table, max_ep_len=20):\n",
        "  state = foolsball.reset()\n",
        "  done = False\n",
        "  episode = []\n",
        "\n",
        "  for _ in range(max_ep_len):\n",
        "    if done:\n",
        "      break\n",
        "    \n",
        "    greedy_action_index = table.loc[state].argmax()\n",
        "    greedy_action = table.columns[greedy_action_index]\n",
        "    next_state, reward, done = foolsball.step(greedy_action)\n",
        "    episode.append([state, greedy_action, reward])\n",
        "    state = next_state\n",
        "  \n",
        "  return episode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOxDZGq9yvky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "collect_greedy_episode_from_returns_tbl(estimated_returns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHNJRWyWFPR2",
        "colab_type": "text"
      },
      "source": [
        "# Step 15\n",
        "## Todo \n",
        "- Implement the loop in the cell below to update the returns table. \n",
        "- The code will be exactly what we used earlier, except that it will use greedy episodes.\n",
        "\n",
        "- Run the next few cells to evaluate the effectiveness.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-NSm78dEZXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ESTIMATED_RETURNS_TBL = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
        "VISITS_COUNTS_TBL = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
        "\n",
        "n_episodes = 1000\n",
        "\n",
        "for i in range(n_episodes):\n",
        "  #Todo: Implement code block to update ESTIMATED_RETURNS_TBL and VISITS_COUNTS_TBL\n",
        "  #Todo: Make sure you are using greedy episodes."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmDqDS6jEa0M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimated_returns = ESTIMATED_RETURNS_TBL.div(VISITS_COUNTS_TBL+1) ## Averaging returns. Avoid dividing by zeros.\n",
        "estimated_returns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_0kWY_vEyDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "policy1 = greedy_policy_from_returns_tbl(estimated_returns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMCzL3AfEl82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretty_print_policy(policy1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXqSmRG_AVWV",
        "colab_type": "text"
      },
      "source": [
        "# Step 16\n",
        "## The Exploration-exploitation Dilemma\n",
        "\n",
        "- We have tried pure exploration (with random episodes)\n",
        "- We have also tried pure exploitation (with policy generated from the returns table)\n",
        "- A good agent should try to balance both.\n",
        "\n",
        "\n",
        "## Epsilon-greedy episodes\n",
        "- An epsilon greedy episode blends the previous two approaches\n",
        "- Precisely, when in state `s`:\n",
        "  - The epsilon greedy episode will pick the action yielding the highest returns with a high probability, say 0.8 \n",
        "  - It will sometime, random action from the other, suboptimal, actions, albeit with a low probability, say 0.2.\n",
        "  - The hyperparameter `epsilon` or $\\epsilon$ decides the probability\n",
        "\n",
        "  - Example with epsilon = 0.2\n",
        "    - state `s`\n",
        "    - Actions = ('n','e','w','s')\n",
        "    - Best action (yielding highest return) = 'w'\n",
        "    - Sampling probabilities = $[1-\\epsilon+{\\epsilon \\over 4},{\\epsilon \\over 4},{\\epsilon \\over 4},{\\epsilon \\over 4}] = [0.85,0.05,0.05,0.05]$\n",
        "\n",
        "\n",
        "## Todo:\n",
        "Finish the code below and look at how the output differs from the other two methods. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "411GuzIjy0Ai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collect_epsilon_greedy_episode_from_returns_tbl(table, max_ep_len=20, epsilon=0.1):\n",
        "  \n",
        "  state = foolsball.reset()\n",
        "  done = False\n",
        "  episode = []\n",
        "\n",
        "  for _ in range(max_ep_len):\n",
        "    if done:\n",
        "      break\n",
        "    \n",
        "    actions = table.columns\n",
        "    action_probs = np.asarray([epsilon/len(actions)]*len(actions),dtype=np.float)\n",
        "    \n",
        "    greedy_action_index = table.loc[state].argmax()\n",
        "    action_probs[greedy_action_index] += 1-epsilon\n",
        "    \n",
        "    epsilon_greedy_action = #Todo: use np.random.choice to sample epsilon-greedily\n",
        "\n",
        "    next_state, reward, done = foolsball.step(epsilon_greedy_action)\n",
        "    episode.append([state, epsilon_greedy_action, reward])\n",
        "    state = next_state\n",
        "\n",
        "  return episode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoOvdylQnmfF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "collect_epsilon_greedy_episode_from_returns_tbl(estimated_returns, epsilon=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzeUyjXYGssT",
        "colab_type": "text"
      },
      "source": [
        "# Step 17\n",
        "## Epsilon-greedy updates.\n",
        "## Todo:\n",
        "- Run the next few cells to see the effect of using an epsilon greedy approach."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDMy2xAcBDZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ESTIMATED_RETURNS_TBL = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
        "VISITS_COUNTS_TBL = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
        "\n",
        "n_episodes = 1000\n",
        "\n",
        "for i in range(n_episodes):\n",
        "  estimated_returns = ESTIMATED_RETURNS_TBL.div(VISITS_COUNTS_TBL+1)\n",
        "  \n",
        "  episode_i = collect_epsilon_greedy_episode_from_returns_tbl(estimated_returns)\n",
        "  #print(episode_i)\n",
        "  states, actions, discounted_returns = discounted_return_from_episode(episode_i, gamma=HYPER_PARAMS['gamma'])\n",
        "\n",
        "  for s,a,ret in zip(states, actions, discounted_returns):\n",
        "    ESTIMATED_RETURNS_TBL.loc[s,a] += ret\n",
        "    VISITS_COUNTS_TBL.loc[s,a] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJM0wX3CEzad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimated_returns = ESTIMATED_RETURNS_TBL.div(VISITS_COUNTS_TBL+1) ## Averaging returns. Avoid dividing by zeros.\n",
        "estimated_returns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHLtgKCyFHsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "policy2 = greedy_policy_from_returns_tbl(estimated_returns)\n",
        "policy2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orYp9J0cHXB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretty_print_policy(policy2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kJ3g6kfHaBN",
        "colab_type": "text"
      },
      "source": [
        "# Step 18\n",
        "## Revisiting Exploration-Exploitation with Epsilon Decay\n",
        "\n",
        "- What is the best way to balance exploitation with exploration?\n",
        "  - In the beginning, pick absolutely random actions in every state.\n",
        "  - Slowly reduce the randomness to a small value.\n",
        "\n",
        "## Todo:\n",
        "- In the code below pick a value of `epsilon` that makes all actions equiprobable in `collect_epsilon_greedy_episode_from_returns_tbl()`.\n",
        "\n",
        "- Fill in the code anneal epsilon over episodes. The value of epsilon shoud not drop below the minimum threshold.\n",
        "\n",
        "- Run the next few cells to evaluate this approach.\n",
        "\n",
        "- Do the policies look any better?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7j3NwXiK7EY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ESTIMATED_RETURNS_TBL = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
        "VISITS_COUNTS_TBL = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
        "\n",
        "n_episodes = 10000\n",
        "epsilon = 1\n",
        "min_epsilon = 0.1\n",
        "epsilon_decay = 0.999\n",
        "\n",
        "for i in range(n_episodes):\n",
        "  estimated_returns = ESTIMATED_RETURNS_TBL.div(VISITS_COUNTS_TBL+1)\n",
        "  \n",
        "  epsilon = #Todo: Pick annealed value unless it is lower than the minimum threshold\n",
        "  episode_i = collect_epsilon_greedy_episode_from_returns_tbl(estimated_returns,epsilon=epsilon)\n",
        "  epsilon *= epsilon_decay\n",
        "  #print(episode_i)\n",
        "  states, actions, discounted_returns = discounted_return_from_episode(episode_i, gamma=HYPER_PARAMS['gamma'])\n",
        "\n",
        "  for s,a,ret in zip(states, actions, discounted_returns):\n",
        "    ESTIMATED_RETURNS_TBL.loc[s,a] += ret\n",
        "    VISITS_COUNTS_TBL.loc[s,a] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFezSR8KNmDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimated_returns = ESTIMATED_RETURNS_TBL.div(VISITS_COUNTS_TBL+1) ## Averaging returns. Avoid dividing by zeros.\n",
        "print(estimated_returns)\n",
        "\n",
        "policy3 = greedy_policy_from_returns_tbl(estimated_returns)\n",
        "print(policy3)\n",
        "\n",
        "pretty_print_policy(policy3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eIjxpr4sZEu",
        "colab_type": "text"
      },
      "source": [
        "# Step 19\n",
        "## Constant Alpha\n",
        "\n",
        "## The idea:\n",
        "- Dividing the accumulated by visit count has a non linear effect on the updates. (Go back to previous step and see for yourself).\n",
        "\n",
        "- Don't divide at all!\n",
        "\n",
        "- But we need to ensure that updates are small\n",
        "\n",
        "  - `ESTIMATED_RETURNS_TBL.loc[s,a]` and `ret` are both estimates of the same quantity. \n",
        "\n",
        "  - Use the difference of the two estimates to update `ESTIMATED_RETURNS_TBL.loc[s,a]` much like we do in Deep Learning.\n",
        "\n",
        "\n",
        "## Todo:\n",
        "- Complete the missing code in the next cell.\n",
        "- Run the next few cells to get a policy and evaluate it.\n",
        "- Does the policy help the agent attain its goal?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joZ5JyEUrb6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ESTIMATED_RETURNS_TBL = pd.DataFrame.from_dict({s:{a:0 for a in foolsball.actions} for s in range(foolsball.n_states)}, orient='index')\n",
        "\n",
        "n_episodes = 10000\n",
        "epsilon = 1\n",
        "min_epsilon = 0.1\n",
        "epsilon_decay = 0.999\n",
        "\n",
        "alpha = 0.01\n",
        "\n",
        "for i in range(n_episodes):\n",
        "  estimated_returns = ESTIMATED_RETURNS_TBL\n",
        "  \n",
        "  epsilon = max(epsilon,min_epsilon)\n",
        "  episode_i = collect_epsilon_greedy_episode_from_returns_tbl(estimated_returns,epsilon=epsilon)\n",
        "  epsilon *= epsilon_decay\n",
        "  states, actions, discounted_returns = discounted_return_from_episode(episode_i, gamma=HYPER_PARAMS['gamma'])\n",
        "\n",
        "  for s,a,ret in zip(states, actions, discounted_returns):\n",
        "    ESTIMATED_RETURNS_TBL.loc[s,a] += #Todo: Update RHS using hints from the instructions. Use alpha as the \"learning rate\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEvhUgzPsmPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimated_returns = ESTIMATED_RETURNS_TBL\n",
        "print(estimated_returns)\n",
        "\n",
        "policy4 = greedy_policy_from_returns_tbl(estimated_returns)\n",
        "print(policy4)\n",
        "\n",
        "pretty_print_policy(policy4)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}